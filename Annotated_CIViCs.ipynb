{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TariqMBrown/CIVICs/blob/main/Annotated_CIViCs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **READ ME!**\n",
        "\n",
        "Hello user. This is a code made by Tariq Brown specifically for processing 2-Photon calcium imaging data, but should work for any 2-dimensional (x,y) time series data.\n",
        "\n",
        "This is the fully annotated version of civics, so it is a little more cluttered, but should explain every step. The red text will explain all the aspects of the code. Green text that has a # before it will also help explain some aspects.\n",
        "\n",
        "Certain aspects are grouped together (it will have a title, and under that title it will have x number of hidden cells, you can run them all together by pressing the play button while the cells are hidden).\n",
        "\n",
        "Use # at the beginning of a line to \"comment\" it out. When you run the cell, that line will be ignored.\n",
        "\n",
        "\n",
        "If you have any issues, please email Tariq at Tariq_Brown@brown.edu or TariqMBrown@gmail.com"
      ],
      "metadata": {
        "id": "lqmNv3vchRcz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWEJBIMyz5oy"
      },
      "source": [
        "# Imports and Definitions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "If you plan on working within google drive, ensure that you mount your drive so you can access your files.\n",
        "after your drive is mounted, you should be able to access your drive files by clicking on the folder\n",
        "icon to the left, and navigating to your working path.  \"drive\" -> \"MyDrive\" -> File Location\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6bN9bD5mp4P_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "These are things not native to the known libraries within colab, so we have to you pip and apt to modify\n",
        "the current colab space. make sure you load these in everytime.\n",
        "\"\"\"\n",
        "%pip install scikit-posthocs\n",
        "%pip install scikit-learn matplotlib\n",
        "!apt-get install -y imagemagick"
      ],
      "metadata": {
        "id": "IUuM5kmotAHA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxLZxxY6uqA6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "These are imports necessary for the codes function. Make sure you run these everytime.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import collections\n",
        "import csv\n",
        "import time\n",
        "from collections import Counter\n",
        "import os.path\n",
        "from os import path\n",
        "import math\n",
        "from statistics import mode\n",
        "from numpy import ones,vstack\n",
        "from numpy.linalg import lstsq\n",
        "import matplotlib as pltc\n",
        "from matplotlib.collections import LineCollection\n",
        "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
        "import tkinter as tk\n",
        "from tkinter import filedialog\n",
        "from tkinter import simpledialog\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "from PIL import Image, ImageDraw\n",
        "from scipy.stats import shapiro\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from scipy.stats import shapiro, levene, f_oneway, kruskal\n",
        "import scikit_posthocs as sp\n",
        "from scipy.stats import f_oneway\n",
        "from scipy.integrate import trapz\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
        "import matplotlib.animation as animation\n",
        "from matplotlib import rc\n",
        "from IPython.display import HTML\n",
        "rc('animation', html='jshtml')\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.cluster.vq import vq, kmeans2\n",
        "import matplotlib.patches as patches\n",
        "from random import seed\n",
        "from matplotlib import cm\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "Secs_in_Min = 60\n",
        "FPS = 31"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Color Selection\n",
        "\"\"\"\n",
        "This is where you can select your preferred colors. Either use matploblib friendly color words, which are listed\n",
        "here: https://matplotlib.org/stable/gallery/color/named_colors.html\n",
        "\n",
        "or use hexidecimal codes.\n",
        "\n",
        "You can find palette creators here: https://coolors.co/d7fff1-aafcb8-8cd790-77af9c-285943\n",
        "and here: http://colormind.io/\n",
        "\"\"\"\n",
        "\n",
        "colors = ['#00539C', '#EEA47F', '#D9D9D9', 'red', 'gray', 'cyan', '#4b4c7e', '#e0dcb6', '#dfdaca']\n",
        "colors2 = ['#4b4c7e', '#e0dcb6', '#dfdaca']\n",
        "palette2 = [\"#808080\",\"#8ed4cc\",\"#205072\",\"#b19cd9\",\"#4682b4\",\"#4b0082\",\"#CCCCFF\",\"#708090\",\"#DB7093\",\"#191970\"]\n",
        "custom_palette = [\"#b3b3b3\",\"#0000a7\",\"#c1272d\",\"#eecc16\",\"#008176\",\"#4b0082\",\"#CCCCFF\",\"#708090\",\"#DB7093\",\"#191970\"]\n",
        "\n",
        "sns.palplot(sns.cubehelix_palette(8))\n",
        "sns.palplot(colors)\n",
        "sns.palplot(colors2)\n",
        "sns.palplot(palette2)\n",
        "sns.palplot(custom_palette)\n",
        "\n",
        "\n",
        "custom_palette = [\"#b3b3b3\",\"#0000a7\",\"#c1272d\",\"#eecc16\",\"#008176\",\"#4b0082\",\"#CCCCFF\",\"#708090\",\"#DB7093\",\"#191970\"]\n"
      ],
      "metadata": {
        "id": "_NGjMLQVz8HV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\"\"\"\n",
        "filepath = '/content/drive/MyDrive/Brown University/Kaun Lab/2-photon Kaun/CIVICs/Output' # change the path to where you want the files to be saved\n",
        "filename = 'Place Holder'\n",
        "project = 'Place Holder'\n",
        "date = 'Place Holder'\n",
        "window_start = 0 # can chnage this to shift your window of interest (start point in frames)\n",
        "window_end = 1426  # can change this to shift your window of interst (end point in frames) 810/806 for my 13 mins videos, 1426 for the 23 min ones\n",
        "stimulus_start = 124 #*62\n",
        "stimulus_end = 744 #744 for 12 min mark\n",
        "stimulus2_start = 744\n",
        "stimulus2_end = 1364 #for acq, 1364\n",
        "baseline_start_1 = stimulus_start - 10\n",
        "baseline_end_1 = stimulus_start - 1\n",
        "baseline_start_2 = stimulus2_start - 10\n",
        "baseline_end_2 = stimulus2_start - 1\n",
        "ybottom = None #if you wanna change the y limit minimum\n",
        "ytop = None #if you wanna change the maximum y limit dispayed\n",
        "Region = 'Insert Region Here' #this can be changed to 'Soma','Axon', 'Dendrite', or 'Other'\n",
        "\n",
        "time_for_bins = 10 #5 second bins default\n",
        "bin_size = math.ceil((window_end - window_start)/time_for_bins)\n",
        "\n",
        "spike_threshold = 0.5\n",
        "p_threshold = 0.05\n",
        "Single_Stim = False\n",
        "Acquisition = True\n",
        "Retrieval = False"
      ],
      "metadata": {
        "id": "06D-EK5IEH1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dC3xw5Mzcm_"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "These are the functions necessary for multiple parts of this code. They will have a comment above them describing their functions\n",
        "\"\"\"\n",
        "\n",
        "\"\"\"\n",
        "column namer - function that iterates through a provided csv and gives it column names in the order of; Soma, Axon, Dendrite, other. So order your csvs accordingly\n",
        "for example, columns 0-3 are all soma, columns 4-5 are axons, and columns 7-9 are dendrites\n",
        "df = pandas dataframe (this will be the column nameless csv that you load in)\n",
        "soma_num = float indicating the number of soma\n",
        "axon_num = float indicating the number of axon\n",
        "dendrite_num = float indicating the number of dendrite\n",
        "other_num = float indicating the number of other structures\n",
        "\n",
        "\"\"\"\n",
        "def column_namer (df, soma_num=None, axon_num=None, dendrite_num=None, other_num=None):\n",
        "    soma_counter = 1\n",
        "    axon_counter = 1\n",
        "    dendrite_counter = 1\n",
        "    other_counter = 1\n",
        "    column_names = [] #need to initalize with the name 'Frame', as to not cause mismatch error\n",
        "    if soma_num > 0:\n",
        "        for i in range(soma_num):\n",
        "            column_names.append(\"Soma\" + ' ' + str(soma_counter))\n",
        "            soma_counter +=1\n",
        "    if axon_num > 0 :\n",
        "        for i in range(axon_num):\n",
        "            column_names.append(\"Axon\" + ' ' + str(axon_counter))\n",
        "            axon_counter +=1\n",
        "    if dendrite_num > 0 :\n",
        "        for i in range(dendrite_num):\n",
        "            column_names.append(\"Dendrite\" + ' ' + str(dendrite_counter))\n",
        "            dendrite_counter +=1\n",
        "    if other_num > 0 :\n",
        "        for i in range(soma_num):\n",
        "            column_names.append(\"Other\" + ' ' + str(other_counter))\n",
        "            other_counter +=1\n",
        "    column_names.insert(0, 'Frame')\n",
        "    df.columns = [column_names]\n",
        "    return df\n",
        "\n",
        "\"\"\"\n",
        "this is the baselining method used for the code.\n",
        "Use the first baseline_align def for acquisition and just EtOH exposure\n",
        "use the second one for retireval experiments\n",
        "df = pandas dataframe (the csv of your data)\n",
        "time 1 (float) = Number representing the start of your desired baseline period\n",
        "time 2 (float) = Number representing the end of your desired baseline period\n",
        "time 3 (float) = Number representing the start of your second desired baseline period\n",
        "time 4 (float) = Number representing the end of your second desired baseline period\n",
        "\"\"\"\n",
        "def Baseline_Align_1(df, time1 = baseline_start_1, time2 = baseline_end_1):\n",
        "    baseline_frame = df.iloc[time1:time2]\n",
        "    mean = baseline_frame.mean()\n",
        "    df =(df - mean)/mean\n",
        "    return df\n",
        "\n",
        "# Example usage:\n",
        "# df is your original DataFrame with a DateTime index\n",
        "# start_date and end_date define the time period for calculating the mean\n",
        "# normalized_df = normalize_dataframe_to_mean(df, '2023-01-01', '2023-12-31')\n",
        "def Baseline_Align_2(df, time1 = baseline_start_1, time2 = baseline_end_1, time3 = baseline_start_2, time4 = baseline_end_2,\n",
        "                   time5 = stimulus_start, time6 = stimulus_end, time7 = stimulus2_start, time8 = stimulus2_end):\n",
        "\n",
        "    baseline_frame = df.iloc[time1:time2]\n",
        "    if time3 and time4 > 0:\n",
        "      baseline_frame2 = df.iloc[time3:time4]\n",
        "      mean2 = baseline_frame2.mean()\n",
        "    mean = baseline_frame.mean()\n",
        "    df.iloc[window_start:time6+15] =(df.iloc[window_start:time6+15] - mean)/mean #+14?\n",
        "    if time3 and time4 >0 :\n",
        "      df.iloc[time7-15:window_end] =(df.iloc[time7-15:window_end] - mean2)/mean2 #\n",
        "    return df\n",
        "\"\"\"\n",
        " This will utlize the baseline_align function to baseline the dataframe, it iterates through each column and baselines\n",
        " df (pandas DataFrame) = csv of your data\n",
        "\"\"\"\n",
        "def Baseline(df):\n",
        "    #print(df)\n",
        "    new_columns = []\n",
        "    for columns in df:\n",
        "        e = (str(columns))\n",
        "        if Acquisition == 1 or Single_Stimulus == 1:\n",
        "          a = pd.DataFrame({e +' DF/F':Baseline_Align_1(df[columns])})\n",
        "        elif Retrieval == 1:\n",
        "          a = pd.DataFrame({e +' DF/F':Baseline_Align_2(df[columns])})\n",
        "        else:\n",
        "          raise ValueError('Please ensure that you have set either Acquisition or Retrieval to True so that the code can baseline properly :)!')\n",
        "        df = pd.concat([df, a], axis = 1)\n",
        "    for columns in df:\n",
        "        new_columns.append(str(columns).replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\").replace(\"'\",\"\"))\n",
        "    df.columns = new_columns\n",
        "    df = df.drop(columns = 'Frame DF/F', axis = 1)\n",
        "    #print(df)\n",
        "    return df\n",
        "\"\"\"\n",
        "All of the following are very poor pieces of code. I am sorry. But it does the job.\n",
        "Region_Avg and Region_SEM returns the mean and average for the region within your dataframe\n",
        "\"\"\"\n",
        "def Soma_Avg(df):\n",
        "    df = df.filter(regex = 'Soma')\n",
        "    mean = df.mean(axis = 1)\n",
        "    return mean\n",
        "def Soma_SEM(df):\n",
        "    df = df.filter(regex = 'Soma')\n",
        "    standard_error = df.sem(axis = 1)\n",
        "    return standard_error\n",
        "def Axon_Avg(df):\n",
        "    df = df.filter(regex = 'Axon')\n",
        "    mean = df.mean(axis = 1)\n",
        "    standard_error = df.sem(axis = 1)\n",
        "    return mean\n",
        "def Axon_SEM(df):\n",
        "    df = df.filter(regex = 'Axon')\n",
        "    standard_error = df.sem(axis = 1)\n",
        "    return standard_error\n",
        "def Dendrite_Avg(df):\n",
        "    df = df.filter(regex = 'Dendrite')\n",
        "    mean = df.mean(axis = 1)\n",
        "    standard_error = df.sem(axis = 1)\n",
        "    return mean\n",
        "def Dendrite_SEM(df):\n",
        "    df = df.filter(regex = 'Dendrite')\n",
        "    standard_error = df.sem(axis = 1)\n",
        "    return standard_error\n",
        "def Other_Avg(df):\n",
        "    df = df.filter(regex = 'Other')\n",
        "    mean = df.mean(axis = 1)\n",
        "    standard_error = df.sem(axis = 1)\n",
        "    return mean\n",
        "def Other_SEM(df):\n",
        "    df = df.filter(regex = 'Other')\n",
        "    standard_error = df.sem(axis = 1)\n",
        "    return standard_error\n",
        "\"\"\"\n",
        "df (pandas DataFrame) = the dataframe that has been baselined will be fed into this function\n",
        "\n",
        "takes the mean and sem of each of the regions\n",
        "\"\"\"\n",
        "def Mean_and_SEM(df):\n",
        "    SomaAvg = Soma_Avg(df)\n",
        "    AxonAvg = Axon_Avg(df)\n",
        "    DendriteAvg = Dendrite_Avg(df)\n",
        "    OtherAvg = Other_Avg(df)\n",
        "    SomaSEM = Soma_SEM(df)\n",
        "    AxonSEM = Axon_SEM(df)\n",
        "    DendriteSEM = Dendrite_SEM(df)\n",
        "    OtherSEM = Other_SEM(df)\n",
        "    MeanSEMdf = pd.DataFrame({\"Frame\": range(1, 1 +len(df)), \"Soma Average\": SomaAvg,\"Axon Average\": AxonAvg,\n",
        "                  \"Dendrite Average\": DendriteAvg,\"Other Average\": OtherAvg, \"Soma SEM\": SomaSEM,\n",
        "                           \"Axon SEM\": AxonSEM, \"Dendrite SEM\": DendriteSEM, \"Other SEM\": OtherSEM }) #for some reason couldnt get the frames to carry over from previous dataframe\n",
        "    return MeanSEMdf\n",
        "\"\"\"\n",
        "unused function. But if you want rasters of your data, you can use this function. Very similar to heatmap\n",
        "df (pandas DataFrame) = baseline_aligned dataframe of your data\n",
        "\"\"\"\n",
        "def rasterize (df):\n",
        "  tempdf = df\n",
        "  # Find the index when each column reaches its maximum value\n",
        "  max_indices = tempdf.idxmax()\n",
        "  sorted_columns = df[max_indices.sort_values().index]\n",
        "  print(sorted_columns)\n",
        "  for column in df.columns:\n",
        "    tempdf[column]\n",
        "    for index, row in tempdf[column].iteritems():\n",
        "      if row < spike_threshold:\n",
        "        tempdf[column] = tempdf[column].replace(row, 0)\n",
        "      else:\n",
        "        tempdf[column] = tempdf[column].replace(row, 1)\n",
        "\n",
        "  positions = tempdf.apply(lambda x: tempdf.index[x == 1])\n",
        "  #print(positions)\n",
        "\n",
        "  # Create raster plot with inverted y-axis to display columns in ascending order\n",
        "  #print(tempdf.index)\n",
        "  plt.eventplot(positions, linelengths=0.75, colors='black')\n",
        "  plt.yticks(range(positions.index.size), positions.index)\n",
        "  plt.gca().invert_yaxis()\n",
        "  return tempdf\n",
        "\"\"\"\n",
        "plotme - function that graphs the entire dataframe provided by default. Can be used to\n",
        "\n",
        "df = pandas dataframe containing baseline aligned fluorescent values produced by the baseline_align function can also be used after its been meaned by mean_and_sem function\n",
        "time1 - time 6 = floats representing time points in seconds\n",
        "ybottom = float representing the min for the y-axis\n",
        "ytop = float representing the max for the y-axis\n",
        "stimname = string representing the name of the stimulation provided\n",
        "color_num = index of the color you want to select based on the colors array at the beginning of the code\n",
        "\"\"\"\n",
        "def plotme(df, filepath, filename, region,\n",
        "          time1, time2, time3=None, time4=None, time5=None, time6=None,\n",
        "          ybottom=None, ytop=None, stimname=None, stim2name=None, color_num = None):\n",
        "    window_of_interest = df.iloc[time1:time2]\n",
        "    roi = region\n",
        "    if region == 'Axon':\n",
        "        n = str(axon_num)\n",
        "    elif region == 'Soma':\n",
        "        n = str(soma_num)\n",
        "    elif region == 'Dendrite':\n",
        "        n = str(dendrite_num)\n",
        "    elif region == 'Other':\n",
        "        n = str(other_num)\n",
        "    tempdf = pd.DataFrame({'Frame':df['Frame'], roi + ' Average': df[roi +' Average']})\n",
        "    new_time_bins = pd.cut(df['Frame'], bin_size)\n",
        "    df['New_Time_Bins'] = new_time_bins\n",
        "    #print(df)\n",
        "    df['Factors'], bins = pd.factorize(df['New_Time_Bins'], sort = True)\n",
        "    g = sns.lineplot(data = df, x = 'Factors', y =  roi + ' Average', errorbar = None,\n",
        "                     color = custom_palette[color_num])\n",
        "    g.set(xlabel = 'Time (secs)', #xlabel = 'Time binned by ' + str(time_for_bins) + ' seconds',\n",
        "          ylabel = 'DF/F',\n",
        "          xlim = (df['Factors'].min(), df['Factors'].max())\n",
        "    )\n",
        "    g.legend()\n",
        "\n",
        "    df = df.groupby('Factors').mean()\n",
        "    error = df[roi + ' SEM']\n",
        "    y = df[roi + ' Average']\n",
        "    g.fill_between(df.index, y-error, y+error, color = custom_palette[color_num], alpha = 0.2)\n",
        "\n",
        "\n",
        "    # x = tempdf['Frame'].values\n",
        "    # y = tempdf[roi+' Average']\n",
        "    # error = df[roi + ' SEM']\n",
        "    # plt.axvspan(time3, time4, facecolor='gray', alpha=0.2)\n",
        "    # plt.axvspan(time5, time6, facecolor='red', alpha=0.2)\n",
        "    # plt.fill_between(x, y-error, y+error, color = 'purple', alpha = 0.2)\n",
        "    # plt.xlim(time1, time2)\n",
        "    # plt.ylim(ybottom, ytop)\n",
        "    # plt.plot(x, y, 'purple')\n",
        "    # plt.plot([window_start,window_end],[0,0], 'k--') # add black line at 0\n",
        "    plt.title(filename+ ' (n = ' + n + ')')\n",
        "    # plt.xlabel('Time (secs)', color='#1C2833')\n",
        "    # plt.ylabel('DF/F', color='#1C2833')\n",
        "    plt.savefig(filepath + '/' + project + '/' + date + '/' +filename + ' graph.pdf', transparent=True)\n",
        "    return filename+'saved to '+ filepath +' !'\n",
        "\n",
        "\n",
        "def heatmap(df, filename, filepath, windowed = [0,0], time1 = None, time2 = None, time3 = None, cmap = None):\n",
        "  \"\"\"\n",
        "  df = pandas dataframe containing the baseline_aligned columns BEFORE they have been averaged by mean_sem function\n",
        "  filename = (string) what would you like your filename to be\n",
        "  filepath (string) string contsaining the location you would like the heatmap pdf saved to\n",
        "  time = (float)\n",
        "  windowed: (bool) if set true, will sort the columns even further based on when the maximum value is (if it occurs in time 1 to midpoint between time 1 and time 2, midpoint time 1/2\n",
        "  to time 2, time 2 to midpoint time 2 to 3, midpoint time 2 to 3 to want to incorporate a function that breaks down the heatmap even further, classifying neurons that reach their max intensity\n",
        "  within the first portion of sitmulus 1, second portion of simulus 1, first portion of stimulus 2, and\n",
        "  \"\"\"\n",
        "  tempdf = df\n",
        "  columns = tempdf.columns\n",
        "  x = tempdf.values\n",
        "  scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "  scaler = scaler.fit_transform(x)\n",
        "  tempdf = pd.DataFrame(scaler)\n",
        "  if windowed[0] == 0:\n",
        "    # Find the index when each column reaches its maximum value\n",
        "    max_indices = tempdf.idxmax()\n",
        "    sorted_columns = tempdf[max_indices.sort_values(ascending = True).index]\n",
        "    sns.heatmap(sorted_columns.T, cmap = cmap)\n",
        "  #if you want to look at early and late for one stimulus\n",
        "  if windowed[0] == 1 and windowed[1] == 0:\n",
        "    #find position at which the max value occurs\n",
        "    max_indices = tempdf.idxmax()\n",
        "    # Categorize columns into two groups based on index value\n",
        "    early_group = [col for col, idx in max_indices.items() if idx <= ((time1+time2)/2)]\n",
        "    late_group = [col for col, idx in max_indices.items() if ((time1+time2)/2) <= idx]\n",
        "    # Sort columns within each group\n",
        "\n",
        "    early_group_sorted = df[early_group].sort_index(axis=1)\n",
        "    early_group_sorted.reset_index\n",
        "\n",
        "    late_group_sorted = df[late_group].sort_index(axis=1)\n",
        "    late_group_sorted.reset_index\n",
        "\n",
        "    sorted_df = pd.concat([early_group_sorted, late_group_sorted])\n",
        "    sns.heatmap(early_group_sorted.T, cmap = cmap)\n",
        "    sns.heatmap(late_group_sorted.T, cmap = cmap)\n",
        "\n",
        "  # [1,1] for acquisition\n",
        "  if windowed[0] == 1 and windowed[1] == 1:\n",
        "    max_indices = tempdf.idxmax()\n",
        "    early1 = [col for col, idx in max_indices.items() if idx <= ((time1+time2)/2)]\n",
        "    late1 = [col for col, idx in max_indices.items() if ((time1+time2)/2)< idx < time2]\n",
        "    early2 = [col for col, idx in max_indices.items() if time2 < idx < ((time2+time3)/2)]\n",
        "    late2 = [col for col, idx in max_indices.items() if ((time2+time3)/2) <= idx]\n",
        "\n",
        "    early1_group_sorted = df[early1].sort_index(axis=1)\n",
        "    late1_group_sorted = df[late1].sort_index(axis=1)\n",
        "    early2_group_sorted = df[early2].sort_index(axis=1)\n",
        "    late2_group_sorted = df[late2].sort_index(axis=1)\n",
        "    # Concatenate the sorted groups back into a single DataFrame\n",
        "    sorted_df = pd.concat([early1_group_sorted, late1_group_sorted, early2_group_sorted, late2_group_sorted])\n",
        "    #sorted_df = sorted_df.reset_index\n",
        "    sns.heatmap(sorted_df.T, cmap = cmap)\n",
        "    #plt.labels = [heatlabels]\n",
        "\n",
        "  plt.axvline(time1, color = 'k')\n",
        "  plt.axvline(time2, color = 'k')\n",
        "  if time3 != None:\n",
        "    plt.axvline(time3, color = 'k')\n",
        "  plt.title(filename)\n",
        "  plt.savefig(filepath + '/' + project + '/' + date + '/' +filename + ' Heatmap.pdf', transparent=True)\n",
        "\n",
        "\"\"\"\n",
        "mean_at_time does exactly what it says. Takes the mean given the provided time points\n",
        "\n",
        "df (pandas DataFrame) = baselined dataframe\n",
        "\"\"\"\n",
        "\n",
        "def mean_at_time(df, time1, time2):\n",
        "  df = df.iloc[time1:time2]\n",
        "  mean = df.mean(axis = 0)\n",
        "  return mean\n",
        "\n",
        "\"\"\"\n",
        "function that takes a given p value and converts it to significance asterisks. To be used when generating plots.\n",
        "pvalue (float) = value associated with the outcome of a statistical test.\n",
        "\"\"\"\n",
        "\n",
        "def convert_pvalue_to_asterisks(pvalue):\n",
        "    if pvalue <= 0.0001:\n",
        "        return \"****\"\n",
        "    elif pvalue <= 0.001:\n",
        "        return \"***\"\n",
        "    elif pvalue <= 0.01:\n",
        "        return \"**\"\n",
        "    elif pvalue <= 0.05:\n",
        "        return \"*\"\n",
        "    return \"ns\"\n",
        "\"\"\"\n",
        "function that normalizes data\n",
        "f (numpy array) = array containing baselined/meaned fluorescent values\n",
        "\"\"\"\n",
        "def norm(f):\n",
        "    return (f - np.min(f))/np.max(f - np.min(f))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uwFytaF1iE2"
      },
      "source": [
        "# Loading in Data and Formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Zrl6G8R1wJl"
      },
      "outputs": [],
      "source": [
        "#Load in your file\n",
        "f = pd.read_csv('Your Filepath Here', header = None) #insert path here\n",
        "#f2 = pd.read_csv('', header = None) # can add more files by inserting there paths here, and adding f2,f3... to the raw_df generation\n",
        "project = 'Test' # name of the project this work belongs to\n",
        "date = '2023-12-04' #YYYY-MM-DD\n",
        "filename = 'Test' # Insert Filename here\n",
        "# try making the directory for the project and data of the data\n",
        "try:\n",
        "   os.makedirs(os.path.join(filepath, project, date))\n",
        "except FileExistsError:\n",
        "   # directory already exists\n",
        "   pass\n",
        "#creates a new dataframe that will be apended to your csv that represents frame of each value\n",
        "frame = pd.DataFrame(range(1, 1+ len(f)))\n",
        "# makes the raw_df of the raw fluorescence values and frames\n",
        "raw_df = pd.concat([frame, f], axis =1)\n",
        "# determine what paradigm you are using, set one of the following three to 'True' and the others to 'False' to select one\n",
        "Single_Stimulus = True\n",
        "Acquisition = False\n",
        "Retrieval = False\n",
        "# Name of Stimulus\n",
        "Stimulus_Name = 'Insert Stimulus Name Here'\n",
        "Stimulus2_Name = 'Insert Second Stimulus\\'s Name Here'\n",
        "\n",
        "\n",
        "# LET US FIRST DEFINE SOME PARAMETERS\n",
        "window_start = 0\n",
        "window_end = 806\n",
        "stimulus_start = 124\n",
        "stimulus_end = 744\n",
        "stimulus2_start = 0\n",
        "stimulus2_end = 0\n",
        "early_stim_end = stimulus_start + 62\n",
        "late_stim_start = stimulus_start - 62\n",
        "early_stim2_end = stimulus2_start + 62\n",
        "late_stim2_start = stimulus2_end - 62\n",
        "\n",
        "#how many you got of the following?\n",
        "# make sure the data is oriented [Soma, Axon, Dendrite, Other] or if you are just using one type, ignore orientation\n",
        "#ensure that the number of columns you feed into the code equals the total number of columns you want labeled\n",
        "soma_num = 0\n",
        "axon_num = 42\n",
        "dendrite_num = 0\n",
        "other_num = 0\n",
        "#determines where baseline(s) start and ends\n",
        "baseline_start_1 = stimulus_start - 11\n",
        "baseline_end_1 = stimulus_start - 1\n",
        "baseline_start_2 = stimulus2_start - 11\n",
        "baseline_end_2 = stimulus2_start - 1\n",
        "#determines stimuli durations, starts, and ends\n",
        "stimulus_duration = [stimulus_start,stimulus_end]\n",
        "stimulus2_duration = [stimulus2_start,stimulus2_end]\n",
        "early_stimulus_duration = [stimulus_start,((stimulus_start+stimulus_end)/2)]\n",
        "late_stimulus_duation = [((stimulus_start+stimulus_end)/2),stimulus_end]\n",
        "early_stimulus2_duration = [stimulus2_start,((stimulus2_start+stimulus2_end)/2)]\n",
        "late_stimulus2_duation = [((stimulus2_start+stimulus2_end)/2),stimulus2_end]\n",
        "total_duration = 806\n",
        "\n",
        "#determine how you want your data binned\n",
        "time_for_bins = 10 #5 second bins default\n",
        "bin_size = math.ceil((window_end - window_start)/time_for_bins)\n",
        "\n",
        "\n",
        "\n",
        "spike_threshold = 0.5 # necessary for ununused rasterize function\n",
        "\n",
        "Region = 'Axon' #axon/soma/dendrite/other\n",
        "df = raw_df.iloc[0:window_end] #248 or 806 or 1426 == this is your window that will be graphed\n",
        "df = column_namer(df, soma_num, axon_num, dendrite_num, other_num)\n",
        "df = Baseline(df)\n",
        "df = df.filter(like='DF/F')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Specify which indicies are for which groups within your dataframe (based on columns)\n",
        "\"\"\"\n",
        "\"If you have any flies that are not responding, or if you want to isolate the ones that are specific to a stimulus drop them from the dataframe\"\n",
        "drop = [] # non responsder\n",
        "df = df.drop(df.columns[drop], axis =1)\n",
        "\n",
        "#if you want labels (optional) for each individual column, insert the proper amount here, and you can choose to use them later\n",
        "mylabels = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3bGfV6sJ0CcV"
      },
      "source": [
        "# Visualization 1: Time Series Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr11krea5Yz6"
      },
      "outputs": [],
      "source": [
        "# @title Individual Traces\n",
        "\"\"\"\n",
        "Plotting each individual trace without the mean\n",
        "\"\"\"\n",
        "\n",
        "df.plot()\n",
        "plt.xlim(0, window_end)\n",
        "plt.ylim(-0.2, 3.0)\n",
        "plt.xlabel('Time (secs)')\n",
        "plt.ylabel('DF/F')\n",
        "plt.title(filename)\n",
        "#change color_background to True if you want to change the background color to highlight when a stimulus is present\n",
        "color_background = False\n",
        "if color_background == True:\n",
        "  plt.fill_between([stimulus_start, stimulus_end], -0.4, 0.6, color = custom_palette[1], alpha = 0.3)\n",
        "  plt.fill_between([stimulus2_start, stimulus2_end], -0.4, 0.6, color = custom_palette[3], alpha = 0.3)\n",
        "plt.legend(loc='center left', labels = mylabels, bbox_to_anchor=(1, 0.5))\n",
        "plt.savefig(filepath + '/' + project + '/' + date + '/' +filename + ' Individual Traces'+ ' graph.pdf', transparent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4QHDTLGe6Ly6"
      },
      "outputs": [],
      "source": [
        "# @title Individual Traces w/ Mean\n",
        "\"\"\"\n",
        "Plotting each individual trace WITH the mean\n",
        "\"\"\"\n",
        "df.plot(color = 'k', alpha = 0.2)\n",
        "mean = df.mean(axis = 1)\n",
        "plt.plot(mean, 'k', linewidth = 3)\n",
        "plt.xlim(0, window_end)\n",
        "plt.ylim(-0.1, 3.0)\n",
        "plt.plot([window_start,window_end],[0,0], 'k--') # add black, dashed line at 0\n",
        "plt.xlabel('Time (secs)')\n",
        "plt.ylabel('DF/F')\n",
        "#change color_background to True if you want to change the background color to highlight when a stimulus is present\n",
        "color_background = False\n",
        "if color_background == True:\n",
        "  plt.fill_between([stimulus_start, stimulus_end], -0.4, 0.6, color = custom_palette[1], alpha = 0.3)\n",
        "  plt.fill_between([stimulus2_start, stimulus2_end], -0.4, 0.6, color = custom_palette[3], alpha = 0.3)\n",
        "plt.title( filename + ' Individual Traces and Mean')\n",
        "plt.legend(loc='center left', labels = mylabels[0:(len(df))], bbox_to_anchor=(1, 0.5))\n",
        "plt.savefig(filepath + '/' + project + '/' + date + '/' +filename + ' Individual Traces and Mean'+ ' graph.pdf', transparent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mc-VLNCo5Xrh",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Heatmap\n",
        "\"\"\"\n",
        "Heatmap function described earlier in code\n",
        "windowed [0,0] for single stim, [0,1] for early and late of one stimulus, [1,1] for acquisition\n",
        "times = times you want to look at\n",
        "cmap can be any of the seaborn heatmaps. 'None' will set it to magma by default, but viridis, crest and others are popular\n",
        "choose a cmap here: https://seaborn.pydata.org/tutorial/color_palettes.html\n",
        "\"\"\"\n",
        "\n",
        "heatmap(df, filename, filepath, windowed = [0,0],\n",
        "        time1 = stimulus_start, time2 = stimulus_end, time3 = None,\n",
        "        cmap = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jFZwh3u6aX8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Plot the Mean and SEM\n",
        "#takes the mean of the baseline_aligned df and plots it\n",
        "mean_df = Mean_and_SEM(df)\n",
        "#plt.plot([0,80],[0,0], 'k--')\n",
        "#plt.plot([stimulus_start/10,stimulus_end/10],[-0.03,-0.03], 'k-')\n",
        "#plt.plot([stimulus2_start/10,stimulus2_end/10],[-0.03,-0.03], 'r--')\n",
        "#plt.xlim(time1, time2)\n",
        "#plt.ylim(-0.05, 1.2)\n",
        "plotme(mean_df, filepath, filename, Region, window_start, window_end, stimulus_start, stimulus_end, stimulus2_start, stimulus2_end, ybottom, ytop,\n",
        "        color_num = 4, transparent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8amfjvrv-FY8",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Area Under the Curve\n",
        "\"\"\"\n",
        "This box looks at Area under the curve using the trapezoidal method\n",
        "\"\"\"\n",
        "\n",
        "#print(df)\n",
        "time_interval = 1  # Adjust this as needed\n",
        "time_column = [i * time_interval for i in range(len(df))]\n",
        "\n",
        "# Add the time column to the DataFrame\n",
        "mean_df['Time'] = time_column\n",
        "# Define the time periods you want to compare\n",
        "if Single_Stimulus == 1:\n",
        "  start_time_period1 = stimulus_start\n",
        "  end_time_period1 = stimulus_start+62\n",
        "  start_time_period2 = stimulus_end-62\n",
        "  end_time_period2 = stimulus_end\n",
        "if Acquisition == 1:\n",
        "  start_time_period1 = stimulus_start\n",
        "  end_time_period1 = stimulus_start+62\n",
        "  start_time_period2 = stimulus_end-62\n",
        "  end_time_period2 = stimulus_end\n",
        "  start_time_period3 = stimulus2_start\n",
        "  end_time_period3 = stimulus2_start+62\n",
        "  start_time_period4 = stimulus2_end-62\n",
        "  end_time_period4 = stimulus2_end\n",
        "if Retrieval == 1:\n",
        "  start_time_period1 = stimulus_start\n",
        "  end_time_period1 = stimulus_end\n",
        "  start_time_period2 = stimulus2_start\n",
        "  end_time_period2 = stimulus2_end\n",
        "\n",
        "\n",
        "# Select the data for the two time periods\n",
        "period1_data = mean_df.iloc[start_time_period1:end_time_period1]\n",
        "period2_data = mean_df.iloc[start_time_period2:end_time_period2]\n",
        "if Acquisition == 1:\n",
        "  period3_data = mean_df.iloc[start_time_period3:end_time_period3]\n",
        "  period4_data = mean_df.iloc[start_time_period4:end_time_period4]\n",
        "\n",
        "# Calculate the area under the curve for each time period\n",
        "area_period1 = trapz(period1_data[Region + ' Average'], x=period1_data['Time'])\n",
        "area_period2 = trapz(period2_data[Region + ' Average'], x=period2_data['Time'])\n",
        "if Acquisition == 1:\n",
        "  area_period3 = trapz(period3_data[Region + ' Average'], x=period3_data['Time'])\n",
        "  area_period4 = trapz(period4_data[Region + ' Average'], x=period4_data['Time'])\n",
        "\n",
        "# Create a plot and fill the areas under the curves with different colors\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(mean_df['Time'], mean_df[Region + ' Average'], label='Time Series', color='blue')\n",
        "plt.fill_between(period1_data['Time'], period1_data[Region + ' Average'], color='green', alpha=0.3, label=f'Area 1: {area_period1:.2f}')\n",
        "plt.fill_between(period2_data['Time'], period2_data[Region + ' Average'], color='orange', alpha=0.3, label=f'Area 2: {area_period2:.2f}')\n",
        "if Acquisition == 1:\n",
        "  plt.fill_between(period3_data['Time'], period3_data[Region + ' Average'], color='green', alpha=0.3, label=f'Area 3: {area_period2:.2f}')\n",
        "  plt.fill_between(period4_data['Time'], period4_data[Region + ' Average'], color='red', alpha=0.3, label=f'Area 4: {area_period2:.2f}')\n",
        "plt.legend()\n",
        "\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('DF/F')\n",
        "plt.title(filename + '/' +' Area Under the Curve')\n",
        "plt.grid(True)\n",
        "plt.savefig(filepath + '/' + project + '/' + date + '/' +filename + ' AUC graph.pdf', transparent=True)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Graphing of multiple time points side by side\n",
        "\"\"\"\n",
        "This is for comparing two areas of the dataframe at once. Change the t# values to set your windows of interest\n",
        "can add more tha 4 ts if you want to compare more than 2 time periods\n",
        "\"\"\"\n",
        "t1 = stimulus_start + 5\n",
        "t2 = stimulus_start + 67\n",
        "t3 = stimulus_end - 62\n",
        "t4 = stimulus_end\n",
        "\n",
        "plt.plot([0,t2-t1],[0,0], 'k--')\n",
        "plotme(mean_df[t1:t2], filepath, filename, Region, window_start, window_end, stimulus_start, stimulus_end, stimulus2_start, stimulus2_end, ybottom, ytop,\n",
        "       color_num = 0)\n",
        "plotme(mean_df[t3:t4], filepath, filename, Region, window_start, window_end, stimulus_start, stimulus_end, stimulus2_start, stimulus2_end, ybottom, ytop,\n",
        "       color_num = 2)\n",
        "plt.savefig(filepath + '/' + project + '/' + date + '/' +filename + ' Comparison graph.pdf', transparent=True)"
      ],
      "metadata": {
        "id": "N8OdPo-dxaSm",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSOIzjvoZMln"
      },
      "source": [
        "# Stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ii1TwBNlZNpz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Stats\n",
        "\n",
        "This portion of the code will look at the designated time points from each of the ROIs in the dataframe\n",
        "and take the mean, max or median for the provided time points and put them in a new DataFrame\n",
        "\"\"\"\n",
        "p_threshold = 0.0\n",
        "sns.despine()\n",
        "t1 = 0\n",
        "t2 = 0\n",
        "t3 = 0\n",
        "t4 = 0\n",
        "t5 = 0\n",
        "t6 = 0\n",
        "t7 = 0\n",
        "t8 = 0\n",
        "\n",
        "\n",
        "\n",
        "if Single_Stimulus == 1:\n",
        "  a = df[0:stimulus_start -1].mean(axis = 0)\n",
        "  b = df[stimulus_start:early_stim_end].mean(axis = 0)\n",
        "  c = df[late_stim_start:stimulus_end].mean(axis = 0)\n",
        "  new_df = pd.concat([a,b,c], axis = 1)\n",
        "  new_df.columns = ['Baseline','Early ' + Stimulus_Name, 'Late ' + Stimulus_Name]\n",
        "elif Acquisition == 1:\n",
        "  a = df[0:stimulus_start -1].mean(axis = 0)\n",
        "  b = df[stimulus_start:early_stim_end].mean(axis = 0)\n",
        "  c = df[late_stim_start:stimulus_end].mean(axis = 0)\n",
        "  d = df[stimulus2_start:early_stim2_end].mean(axis = 0)\n",
        "  e = df[late_stim2_start:stimulus2_end].mean(axis = 0)\n",
        "  new_df = pd.concat([a,b,c,d,e], axis = 1)\n",
        "  new_df.columns = ['Baseline','Early Odor 1', 'Late Odor 1',\n",
        "                  'Early Odor 2 + Ethanol', 'Late Odor 2 + Ethanol']\n",
        "elif Retrieval == 1:\n",
        "  a = df[0:stimulus_start-1].mean(axis = 0)\n",
        "  b = df[stimulus_start:early_stim_end].mean(axis = 0 )\n",
        "  c = df[late_stim_start:stimulus_end].mean(axis = 0)\n",
        "  d = df[stimulus2_start:early_stim2_end].mean(axis = 0)\n",
        "  e = df[late_stim2_start:stimulus2_end].mean(axis = 0)\n",
        "  new_df = pd.concat([a,b,c], axis = 1)\n",
        "  new_df.columns = ['Baseline','Unpaired Odor','Paired Odor']\n",
        "\n",
        "new_df = new_df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLDzOdozc18z",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Single Stimulus Stats Table\n",
        "\n",
        "#now that the dataframe is formed, lets run stats!\n",
        "stats_df = pd.DataFrame({})\n",
        "shapiro_p_value = []\n",
        "levene_p_value = []\n",
        "anova_p_value = []\n",
        "nemenyi_p_value = []\n",
        "tukey_p_value = []\n",
        "for column in new_df.iloc[:, 1:len(new_df)]:\n",
        "    plt.figure()\n",
        "    _, shap_value = shapiro(new_df[column])\n",
        "    shapiro_p_value.append(shap_value)\n",
        "    new_df[column].plot(kind='hist', density=True, edgecolor='black', title=f'Histogram for {column}')\n",
        "    new_df[column].plot(kind='kde', title=f'Density Plot for {column} , p = ' + str(shap_value))\n",
        "    print(f\"Shapiro-Wilk test p-value for '{column}': {shap_value}\")\n",
        "    plt.show()\n",
        "    plt.savefig(filepath + '/' +  project + '/' + date + '/' +filename + ' ' + str(column) + ' normality plot.pdf')\n",
        "#Calculate p value for equivalence of distribution\n",
        "lev_stat, lev_p = stats.levene(a,b,c)\n",
        "levene_p_value.append(lev_p)\n",
        "print('Your Levene p is ' + str(lev_p) + ' If it is less than 0.05 the variance between the two populations is very high and you should use non parametric test')\n",
        "\n",
        "\n",
        "\n",
        "#Calculate p value and fstat for anova\n",
        "if lev_p > p_threshold and min(shapiro_p_value) > p_threshold:\n",
        "  f_stat, anova_p = f_oneway(new_df['Baseline'],new_df['Early ' + Stimulus_Name],new_df['Late '+ Stimulus_Name])\n",
        "  anova_p_value.append(anova_p)\n",
        "  if anova_p < p_threshold:\n",
        "    combined_df = pd.concat([new_df['Baseline'],new_df['Early '+ Stimulus_Name],new_df['Late '+ Stimulus_Name]], ignore_index = True)\n",
        "    tukey_results = pairwise_tukeyhsd(combined_df.melt())\n",
        "    tukey_p_value.append(tukey_results)\n",
        "  stats_table = {'Shapiro':shapiro_p_value,\n",
        "                       'Levene':levene_p_value,\n",
        "                       'Anova':anova_p_value,\n",
        "                       'Tukey': tukey_p_value}\n",
        "\n",
        "else:\n",
        "  f_stat, anova_p = stats.friedmanchisquare(new_df['Baseline'],new_df['Early '+ Stimulus_Name],new_df['Late '+ Stimulus_Name])\n",
        "  anova_p_value.append(anova_p)\n",
        "\n",
        "  if anova_p < p_threshold:\n",
        "    data = np.array([new_df['Baseline'], new_df['Early '+ Stimulus_Name], new_df['Late '+ Stimulus_Name]])\n",
        "    nemenyi_p = sp.posthoc_nemenyi_friedman(data.T)\n",
        "    print('your nemenyi_p stat is = ' + str(nemenyi_p))\n",
        "  stats_table = {'Shapiro':shapiro_p_value,\n",
        "                       'Levene':levene_p_value,\n",
        "                       'Anova':anova_p_value,\n",
        "                       'Nemenyi': nemenyi_p}\n",
        "print('your f stat is = ' + str(f_stat), 'your anova p value is = ' + str(anova_p))\n",
        "\n",
        "\n",
        "print(stats_table)\n",
        "\n",
        "try:\n",
        "    geeky_file = open(filepath + '/' + project + '/' + date + '/' +filename + ' stats.txt', 'wt')\n",
        "    geeky_file.write(str(stats_table))\n",
        "    geeky_file.close()\n",
        "\n",
        "except:\n",
        "    print(\"Unable to write to file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldaciEbsZfSg",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Acquisition Stats Table\n",
        "stats_df = pd.DataFrame({})\n",
        "shapiro_p_value = []\n",
        "levene_p_value = []\n",
        "anova_p_value = []\n",
        "nemenyi_p_value = []\n",
        "tukey_p_value = []\n",
        "for column in new_df.iloc[:, 1:len(new_df)]:\n",
        "    plt.figure()\n",
        "    _, shap_value = shapiro(new_df[column])\n",
        "    shapiro_p_value.append(shap_value)\n",
        "    new_df[column].plot(kind='hist', density=True, edgecolor='black', title=f'Histogram for {column}')\n",
        "    new_df[column].plot(kind='kde', title=f'Density Plot for {column} , p = ' + str(shap_value))\n",
        "    print(f\"Shapiro-Wilk test p-value for '{column}': {shap_value}\")\n",
        "    plt.show()\n",
        "    plt.savefig(filepath + '/' + project + '/' + date + '/' +filename + ' ' + str(column) + ' normality plot.pdf')\n",
        "#Calculate p value for equivalence of distribution\n",
        "lev_stat, lev_p = stats.levene(a,b,c)\n",
        "levene_p_value.append(lev_p)\n",
        "print('Your Levene p is ' + str(lev_p) + ' If it is less than 0.05 the variance between the two populations is very high and you should use non parametric test')\n",
        "\n",
        "\n",
        "\n",
        "#Calculate p value and fstat for anova\n",
        "if lev_p > p_threshold and min(shapiro_p_value) > p_threshold:\n",
        "  f_stat, anova_p = f_oneway(new_df['Baseline'],new_df['Early Odor 1'],new_df['Late Odor 1'],\n",
        "                             new_df['Early Odor 2 + Ethanol'],new_df['Late Odor 2 + Ethanol'])\n",
        "  anova_p_value.append(anova_p)\n",
        "  if anova_p < p_threshold:\n",
        "    combined_df = pd.concat([new_df['Baseline'],new_df['Early Odor 1'],new_df['Late Odor 1'],\n",
        "                            new_df['Early Odor 2 + Ethanol'], new_df['Late Odor 2 + Ethanol']], ignore_index = True)\n",
        "    tukey_results = pairwise_tukeyhsd(combined_df.melt())\n",
        "    tukey_p_value.append(tukey_results)\n",
        "  stats_table = {'Shapiro':shapiro_p_value,\n",
        "                       'Levene':levene_p_value,\n",
        "                       'Anova':anova_p_value,\n",
        "                       'Tukey': tukey_p_value}\n",
        "\n",
        "else:\n",
        "  f_stat, anova_p = stats.stats.friedmanchisquare(new_df['Baseline'],new_df['Early Odor 1'],new_df['Late Odor 1'],\n",
        "                                        new_df['Early Odor 2 + Ethanol'], new_df['Late Odor 2 + Ethanol'])\n",
        "  anova_p_value.append(anova_p)\n",
        "  if anova_p < p_threshold:\n",
        "    data = np.array([new_df['Baseline'], new_df['Early Odor 1'],\n",
        "                     new_df['Late Odor 1'], new_df['Early Odor 2 + Ethanol'],\n",
        "                      new_df['Late Odor 2 + Ethanol']])\n",
        "    nemenyi_p = sp.posthoc_nemenyi_friedman(data.T)\n",
        "    nemenyi_p_value.append(nemenyi_p)\n",
        "    print('your nemenyi p stat is = ' + str(nemenyi_p_value))\n",
        "  stats_table = {'Shapiro':shapiro_p_value,\n",
        "                       'Levene':levene_p_value,\n",
        "                       'Anova':anova_p_value,\n",
        "                       'Nemenyi':nemenyi_p_value}\n",
        "print('your f stat is = ' + str(f_stat), 'your anova p value is = ' + str(anova_p))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(stats_table)\n",
        "\n",
        "try:\n",
        "    geeky_file = open(filepath + '/' + project + '/' + date + '/' +filename + ' stats.txt', 'wt')\n",
        "    geeky_file.write(str(stats_table))\n",
        "    geeky_file.close()\n",
        "\n",
        "except:\n",
        "    print(\"Unable to write to file\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSGbld5AdoF5",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Retrieval Stats Table\n",
        "stats_df = pd.DataFrame({})\n",
        "shapiro_p_value = []\n",
        "levene_p_value = []\n",
        "anova_p_value = []\n",
        "nemenyi_p_value = []\n",
        "tukey_p_value = []\n",
        "for column in new_df.iloc[:, 1:len(new_df)]:\n",
        "    plt.figure()\n",
        "    _, shap_value = shapiro(new_df[column])\n",
        "    shapiro_p_value.append(shap_value)\n",
        "    new_df[column].plot(kind='hist', density=True, edgecolor='black', title=f'Histogram for {column}')\n",
        "    new_df[column].plot(kind='kde', title=f'Density Plot for {column} , p = ' + str(shap_value))\n",
        "    print(f\"Shapiro-Wilk test p-value for '{column}': {shap_value}\")\n",
        "    plt.show()\n",
        "    plt.savefig(filepath + '/' + project + '/' + date + '/' +filename + ' ' + str(column) + ' normality plot.pdf')\n",
        "#Calculate p value for equivalence of distribution\n",
        "lev_stat, lev_p = stats.levene(a,b,c)\n",
        "levene_p_value.append(lev_p)\n",
        "print('Your Levene p is ' + str(lev_p) + ' If it is less than 0.05 the variance between the two populations is very high and you should use non parametric test')\n",
        "\n",
        "\n",
        "\n",
        "#Calculate p value and fstat for anova\n",
        "if lev_p > p_threshold and min(shapiro_p_value) > p_threshold:\n",
        "  f_stat, anova_p = f_oneway(new_df['Baseline'],new_df['Unpaired'],new_df['Paired'])\n",
        "  anova_p_value.append(anova_p)\n",
        "  if anova_p < p_threshold:\n",
        "    combined_df = pd.concat([new_df['Baseline'],new_df['Unpaired'],new_df['Paired']], ignore_index = True)\n",
        "    tukey_results = pairwise_tukeyhsd(combined_df.melt())\n",
        "    tukey_p_value.append(tukey_results)\n",
        "  stats_table = {'Shapiro':shapiro_p_value,\n",
        "                       'Levene':levene_p_value,\n",
        "                       'Anova':anova_p_value,\n",
        "                       'Tukey': tukey_p_value}\n",
        "else:\n",
        "  print('Data is not normal, Friedman Chi Square Test is being used instead of Anova and Tukey')\n",
        "  f_stat, anova_p = stats.friedmanchisquare(new_df['Baseline'],new_df['Unpaired'],new_df['Paired'] )\n",
        "  anova_p_value.append(anova_p)\n",
        "  if anova_p < p_threshold:\n",
        "    data = np.array([new_df['Baseline'], new_df['Unpaired'], new_df['Paired']])\n",
        "    nemenyi_p = sp.posthoc_nemenyi_friedman(data.T)\n",
        "    nemenyi_p_value.append(nemenyi_p)\n",
        "    print('your nemenyi_p stat is = ' + str(nemenyi_p))\n",
        "  stats_table = {'Shapiro':shapiro_p_value,\n",
        "                       'Levene':levene_p_value,\n",
        "                       'Anova':anova_p_value,\n",
        "                       'Nemenyi': nemenyi_p}\n",
        "print('your f stat is = ' + str(f_stat), 'your anova p value is = ' + str(anova_p))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(stats_table)\n",
        "\n",
        "try:\n",
        "    geeky_file = open(filepath + '/' + project + '/' + date + '/' +filename + ' stats.txt', 'wt')\n",
        "    geeky_file.write(str(stats_table))\n",
        "    geeky_file.close()\n",
        "\n",
        "except:\n",
        "    print(\"Unable to write to file\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_JAQNGrhBjT"
      },
      "source": [
        "# Violin Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xd5npjGvhFj1",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Single Stimulus\n",
        "\"\"\"\n",
        "This code chunk is for generating violin/boxplots for etoh only\n",
        "\"\"\"\n",
        "#df.plot()\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plt.setp(ax.collections, alpha=0.4)\n",
        "## if want black\n",
        "fig.patch.set_facecolor('black')  # Set the background color of the figure\n",
        "ax.set_facecolor('black')          # Set the background color of the axis\n",
        "ax.tick_params(axis='both', colors='white')\n",
        "\n",
        "\n",
        "ax = sns.violinplot(data = new_df, width = 1, palette = custom_palette, linewidth = 0.5, inner = 'quart', gap = 0)\n",
        "\"\"\"\n",
        "use swarmplot for just adding randomly spaced individual dots\n",
        "use sns.lineplot for rigidly structured and connected dots across time periods\n",
        "\"\"\"\n",
        "# ax = sns.swarmplot(data = new_df, palette = custom_palette,\n",
        "#               color=\"0.1\", alpha = 0.5, size = 5,\n",
        "#               linewidth = 2)\n",
        "## Do this if you want to add individual dots with lines connecting them\n",
        "counter = 1\n",
        "for n in df.columns[1:]:\n",
        "  sns.lineplot(y = [new_df['Baseline'][counter], new_df['Early ' +Stimulus_Name][counter], new_df['Late ' + Stimulus_Name][counter]],\n",
        "               x = ['Baseline', 'Early ' + Stimulus_Name, 'Late ' +Stimulus_Name], alpha = 0.5, color = 'w')\n",
        "  sns.lineplot(y = [new_df['Baseline'][counter]],\n",
        "               x = ['Baseline'], alpha = 0.5, marker = 'o', color = custom_palette[0])\n",
        "  sns.lineplot(y = [new_df['Early ' +Stimulus_Name][counter]],\n",
        "               x = ['Early ' + Stimulus_Name], alpha = 0.5, marker = 'o', color = custom_palette[1])\n",
        "  sns.lineplot(y = [new_df['Late ' +Stimulus_Name][counter]],\n",
        "               x = ['Late ' +Stimulus_Name], alpha = 1, marker = 'o', color = custom_palette[2])\n",
        "  counter = counter + 1\n",
        "\n",
        "\n",
        "\n",
        "sns.despine(left=True, bottom = True)\n",
        "plt.tick_params(left = False, bottom = False)\n",
        "\n",
        "#plt.figure(figsize=(10, 6))\n",
        "plt.title(filename + ' Boxplot' + ', p-value = ' +str(anova_p), color = 'w')\n",
        "plt.xlabel(\"Time Period\", color = 'w')\n",
        "plt.ylabel(\"DF/F\", color = 'w')\n",
        "#plt.ylim((-0.2,0.75))\n",
        "\n",
        "\n",
        "\n",
        "#plt.show()\n",
        "plt.savefig(filepath + '/'+ project +  '/' + date +'/' +filename + ' violinplot.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca7AYvLrhHzD",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Acquisition of Alcohol Memory\n",
        "\"\"\"\n",
        "This code chunk is for generating violin/boxplots for acquisition\n",
        "\"\"\"\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plt.setp(ax.collections, alpha=0.4)\n",
        "## if want black\n",
        "fig.patch.set_facecolor('black')  # Set the background color of the figure\n",
        "ax.set_facecolor('black')          # Set the background color of the axis\n",
        "ax.tick_params(axis='both', colors='white')\n",
        "\n",
        "\n",
        "ax = sns.violinplot(data = new_df, width = 1, palette = custom_palette, linewidth = 0.5, inner = 'quart', gap = 0)\n",
        "\"\"\"\n",
        "use swarmplot for just adding randomly spaced individual dots\n",
        "use sns.lineplot for rigidly structured and connected dots across time periods\n",
        "\"\"\"\n",
        "# ax = sns.swarmplot(data = new_df, palette = custom_palette2,\n",
        "#               color=\"0.1\", alpha = 0.5, size = 5,\n",
        "#               linewidth = 2)\n",
        "## Do this if you want to add individual dots with lines connecting them\n",
        "counter = 1\n",
        "for n in df.columns[1:]:\n",
        "  sns.lineplot(y = [new_df['Baseline'][counter], new_df['Early Odor 1'][counter], new_df['Late Odor 1'][counter],\n",
        "               new_df['Early Odor 2 + Ethanol'][counter], new_df['Late Odor 2 + Ethanol'][counter]],\n",
        "               x = ['Baseline', 'Early Odor 1', 'Late Odor 1', 'Early Odor 2 + Ethanol', 'Late Odor 2 + Ethanol'], alpha = 0.5, color = 'w')\n",
        "  sns.lineplot(y = [new_df['Baseline'][counter]],\n",
        "               x = ['Baseline'], alpha = 0.5, marker = 'o', color = custom_palette[0])\n",
        "  sns.lineplot(y = [new_df['Early Odor 1'][counter]],\n",
        "               x = ['Early Odor 1'], alpha = 0.5, marker = 'o', color = custom_palette[1])\n",
        "  sns.lineplot(y = [new_df['Late Odor 1'][counter]],\n",
        "               x = ['Late Odor 1'], alpha = 1, marker = 'o', color = custom_palette[2])\n",
        "  sns.lineplot(y = [new_df['Early Odor 2 + Ethanol'][counter]],\n",
        "               x = ['Early Odor 2 + Ethanol'], alpha = 1, marker = 'o', color = custom_palette[3])\n",
        "  sns.lineplot(y = [new_df['Late Odor 2 + Ethanol'][counter]],\n",
        "               x = ['Late Odor 2 + Ethanol'], alpha = 1, marker = 'o', color = custom_palette[4])\n",
        "  counter = counter + 1\n",
        "\n",
        "\n",
        "sns.despine(left=True, bottom = True)\n",
        "plt.tick_params(left = False, bottom = False)\n",
        "\n",
        "#plt.figure(figsize=(10, 6))\n",
        "plt.title(filename + ' Boxplot' + ', Kruskal-Wallis p-value = ' +str(anova_p), color = 'w')\n",
        "plt.xlabel(\"Time Period\", color = 'w')\n",
        "plt.ylabel(\"DF/F\", color = 'w')\n",
        "plt.ylim((-0.4,1.2))\n",
        "\n",
        "\n",
        "\n",
        "#plt.show()\n",
        "plt.savefig(filepath + '/'+ project +  '/' + date +'/' +filename + ' violinplot.pdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p0f8FLwh6bk",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Retrieval Violin Plots\n",
        "\"\"\"\n",
        "This code chunk is for generating violin/boxplots for Retrieval\n",
        "\"\"\"\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "plt.setp(ax.collections, alpha=0.4)\n",
        "## if want black\n",
        "fig.patch.set_facecolor('black')  # Set the background color of the figure\n",
        "ax.set_facecolor('black')          # Set the background color of the axis\n",
        "ax.tick_params(axis='both', colors='white')\n",
        "\n",
        "\n",
        "ax = sns.violinplot(data = new_df, width = 1, palette = custom_palette, linewidth = 0.5, inner = 'quart', gap = 0)\n",
        "\"\"\"\n",
        "use swarmplot for just adding randomly spaced individual dots\n",
        "use sns.lineplot for rigidly structured and connected dots across time periods\n",
        "\"\"\"\n",
        "# ax = sns.swarmplot(data = new_df, palette = custom_palette2,\n",
        "#               color=\"0.1\", alpha = 0.5, size = 5,\n",
        "#               linewidth = 2)\n",
        "## Do this if you want to add individual dots with lines connecting them\n",
        "counter = 1\n",
        "for n in df.columns[1:]:\n",
        "  sns.lineplot(y = [new_df['Baseline'][counter], new_df['Unpaired'][counter], new_df['Paired'][counter]],\n",
        "               x = ['Baseline', 'Unpaired', 'Paired'], alpha = 0.5, color = 'w')\n",
        "  sns.lineplot(y = [new_df['Baseline'][counter]],\n",
        "               x = ['Baseline'], alpha = 0.5, marker = 'o', color = custom_palette[0])\n",
        "  sns.lineplot(y = [new_df['Unpaired'][counter]],\n",
        "               x = ['Unpaired'], alpha = 0.5, marker = 'o', color = custom_palette[7])\n",
        "  sns.lineplot(y = [new_df['Paired'][counter]],\n",
        "               x = ['Paired'], alpha = 1, marker = 'o', color = custom_palette[2])\n",
        "  counter = counter + 1\n",
        "\n",
        "\n",
        "sns.despine(left=True, bottom = True)\n",
        "plt.tick_params(left = False, bottom = False)\n",
        "\n",
        "#plt.figure(figsize=(10, 6))\n",
        "plt.title(filename + ' Boxplot' + ', Kruskal-Wallis p-value = ' +str(anova_p), color = 'w')\n",
        "plt.xlabel(\"Time Period\", color = 'w')\n",
        "plt.ylabel(\"DF/F\", color = 'w')\n",
        "plt.ylim((-0.2,0.75))\n",
        "\n",
        "\n",
        "\n",
        "#plt.show()\n",
        "plt.savefig(filepath + '/'+ project +  '/' + date +'/' +filename + ' violinplot.pdf')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJwqNTLI5qIt"
      },
      "source": [
        "# Fancy Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k0U8d0a65q0v"
      },
      "outputs": [],
      "source": [
        "# @title Frequency Plots\n",
        "\"\"\"\n",
        "Creates a histogram comparing two time periods that you designate\n",
        "times should be in seconds\n",
        "\"\"\"\n",
        " # this is just to set up the graph\n",
        "%matplotlib inline\n",
        "plt.rcParams.update({'figure.figsize':(7,5), 'figure.dpi':100})\n",
        "params = {\"ytick.color\" : \"k\",\n",
        "          \"xtick.color\" : \"k\",\n",
        "          \"axes.labelcolor\" : \"k\",\n",
        "          \"axes.edgecolor\" : \"k\"}\n",
        "plt.rcParams.update(params)\n",
        "title = 'Test'\n",
        "fig.patch.set_facecolor('white') #or black\n",
        "\n",
        "t1 = stimulus_start\n",
        "t2 = stimulus_start + 62\n",
        "t3 = stimulus_end - 62\n",
        "t4 = stimulus_end\n",
        "t5 = 0\n",
        "t6 = 0\n",
        "t7 = 0\n",
        "t8 = 0\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "could add multiple x's to increase number of things you want to plot\n",
        "\"\"\"\n",
        "# def counts_histogram(df, filepath, title, t1 = None, t2 = None, t3 = None\n",
        "#                      t4 = None, t5 = None, t6 = None)\n",
        "\n",
        "x = mean_df[Region + ' Average'].iloc[t1:t2]\n",
        "x2 = mean_df[Region + ' Average'].iloc[t3:t4]\n",
        "#x3 = df[roi + 'Average'].iloc[t5:t6]\n",
        "fig = plt.figure()\n",
        "#fig.spines[['right', 'top']].set_visible(False)\n",
        "plt.hist(x, bins=20, alpha = 0.5, label = 'Early ' + Stimulus_Name, color = 'blue')\n",
        "plt.hist(x2, bins=20, alpha = 0.5, label = 'Late ' + Stimulus_Name, color = 'indigo')\n",
        "plt.legend(labelcolor = 'linecolor')\n",
        "plt.rcParams['axes.facecolor'] = 'w' #or black or white\n",
        "plt.title('Insert Title Here', color = 'k')\n",
        "plt.gca().set(xlabel = '\\u0394F/F' ,ylabel='Density');\n",
        "plt.savefig(filepath + '/' + project + '/' + date + '/' + title+ ' Frequency graph.pdf', transparent = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F0lzrvqm7He7"
      },
      "outputs": [],
      "source": [
        "# @title Time Series Gifs\n",
        "\"\"\"\n",
        "Work in progress, was working until fill between stopped the code from working.\n",
        "\"\"\"\n",
        "\n",
        "# Function to update the plot for each frame\n",
        "\n",
        "sem_values = mean_df[Region + ' SEM']\n",
        "def update(frame):\n",
        "    plt.clf()  # Clear previous plot\n",
        "    plt.plot(mean_df['Frame'][:frame + 1], mean_df[Region + ' Average'][:frame + 1], color = 'k')\n",
        "    plt.fill_between(mean_df['Frame'][:frame + 1],\n",
        "                     mean_df[Region + ' Average'][:frame + 1] - sem_values,\n",
        "                     mean_df[Region + ' Average'][:frame + 1] + sem_values,\n",
        "                     color='gray', alpha=0.3, label='SEM')\n",
        "    plt.title(f'Time: {frame}')\n",
        "    plt.xlabel('Time (seconds)')\n",
        "    plt.ylabel('DF/F')\n",
        "\n",
        "# Create the animation\n",
        "animation = FuncAnimation(plt.gcf(), update, frames=len(df), interval=window_end, repeat=False)\n",
        "\n",
        "# Save the animation as a GIF (optional)\n",
        "animation.save(filepath + '/' + filename +  ' Mean.gif', writer='pillow')\n",
        "\n",
        "# Display the animation\n",
        "HTML(animation.to_jshtml())\n",
        "\n",
        "#takes 6 minutes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title K-means clustering of cells\n",
        "# Load your single-cell activity traces data (replace this with your data)\n",
        "# For example, if your data is in a CSV file:\n",
        "# data = np.genfromtxt('your_data.csv', delimiter=',')\n",
        "\n",
        "# Sample data (replace this with your actual data)\n",
        "\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "normalized_data = scaler.fit_transform(df)\n",
        "\n",
        "# Determine the optimal number of clusters (K) using the elbow method\n",
        "wcss = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "    kmeans.fit(normalized_data)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "\n",
        "# Plot the elbow method\n",
        "plt.plot(range(1, 11), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of Clusters (K)')\n",
        "plt.ylabel('WCSS (Within-Cluster Sum of Squares)')\n",
        "plt.show()\n",
        "\n",
        "# Choose the optimal K and perform K-means clustering\n",
        "optimal_k = 5  # replace this with the optimal K from the elbow method\n",
        "kmeans = KMeans(n_clusters=optimal_k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
        "clusters = kmeans.fit_predict(normalized_data)\n",
        "\n",
        "# Now 'clusters' contains the cluster assignments for each cell\n",
        "print(\"Cluster Assignments:\", clusters)"
      ],
      "metadata": {
        "id": "nIun8CVvaXoo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "plots heatmap both unclustered and clustered\n",
        "\"\"\"\n",
        "data_t = np.asarray(df.iloc[:,:].T);\n",
        "plt.figure(figsize=(14,14))\n",
        "ax1 = plt.subplot(1,2,1)\n",
        "plt.imshow(data_t)\n",
        "ax1.set_aspect(20)\n",
        "plt.title('Original traces');\n",
        "\n",
        "ax2 = plt.subplot(1,2,2)\n",
        "\n",
        "Ks = 5\n",
        "# K-means relies on random number generation, we can fix the seed to have same result each time\n",
        "centroid, labels = kmeans2(data_t, Ks, seed=1111111)\n",
        "\n",
        "# argsort outputs indeces after sorting the argument\n",
        "# so i_labels contains indeces of cells, sorted by corresponding cluster ID\n",
        "i_labels = np.argsort(labels)\n",
        "\n",
        "plt.imshow(data_t[i_labels,:])\n",
        "ax2.set_aspect(20)\n",
        "plt.title('Traces sorted by K-means');\n",
        "\n",
        "cmap = cm.get_cmap('terrain', Ks)\n",
        "\n",
        "# Cosmetic code to create a Rectangle patches to label specific K-cluster\n",
        "Koffset = 0\n",
        "for Ki in range(Ks):\n",
        "    Nk = np.size(np.where(labels == Ki))\n",
        "    # 40 is width of the rectangle\n",
        "    rect = patches.Rectangle((0, Koffset), 50, Nk, linewidth=1, edgecolor='none', facecolor=cmap(Ki))\n",
        "    ax2.text( 10, Koffset + Nk/2, Ki ,color='k', weight='bold')\n",
        "    # Add the patch to the plot\n",
        "    ax2.add_patch(rect)\n",
        "    Koffset += Nk\n",
        "\n",
        "ax2.text(10,-5,'↓ Cluster ID',fontsize=10)\n",
        "\n",
        "\n",
        "# add subplot labels\n",
        "# ax1.text(-200,-10,'A',fontsize=15)\n",
        "# ax2.text(-200,-10,'B',fontsize=15)\n",
        "\n",
        "ax1.set_xlabel('Frame')\n",
        "ax2.set_xlabel('Frame')\n",
        "\n",
        "ax1.set_ylabel('Cell')\n",
        "ax2.set_ylabel('Cell')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kDTDEGY9nBNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(20,5))\n",
        "plt.subplot(121)\n",
        "for Ki in range(0,Ks):\n",
        "    # find indeces of traces where cluster label equals to Ki\n",
        "    js = np.where(labels == Ki);\n",
        "    # calculate average activity trace for cluster Ki\n",
        "    d  = np.mean(np.squeeze(data_t[js, :]), axis=0);\n",
        "    # normalize so that each trace is between 0 and 1\n",
        "    d  = norm(d);\n",
        "    plt.plot(d+Ki)\n",
        "\n",
        "p = plt.gca();\n",
        "p.set_ylabel('Cluster ID', fontsize=15)\n",
        "p.set_xlabel('Frame', fontsize=15)\n",
        "p.set_title('Average activity of each cluster', fontsize=20)\n",
        "\n",
        "# add subplot label for figure\n",
        "#plt.text(-200,6.5,'A',fontsize=20)\n",
        "\n",
        "\n",
        "plt.subplot(122)\n",
        "\n",
        "K0 = 1;\n",
        "iis = np.where(labels == K0)[0];\n",
        "m = np.mean(data_t[iis,:], 0)\n",
        "s = stats.sem(data_t[iis,:], 0)\n",
        "ts = range(0,window_end)\n",
        "plt.plot(m,'-k')\n",
        "# create fill between mean-std and mean+std, fill with grey color\n",
        "plt.fill_between(ts, m-s, m+s, alpha=0.4, color=(0.1,0.1,0.1))\n",
        "p = plt.gca();\n",
        "\n",
        "# Here we use LaTeX symbol for plus/minus sign\n",
        "p.set_title('Average Activity of Cluster #' +str(K0)+  ' (mean $\\pm$ sem)', fontsize=20)\n",
        "p.set_xlabel('Frame', fontsize=15)\n",
        "p.set_ylabel('Activity, a.u.', fontsize=15)\n",
        "\n",
        "p.legend(('Mean','SEM'), fontsize=15)\n",
        "plt.text(-200,0.8,'B',fontsize=20)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "GbKVaYY1lEoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# perform t-SNE dimentionality reduction\n",
        "# t-SNE relies or random number generation, so we fix random_state to have same result each time we run the code\n",
        "X_embedded = TSNE(n_components=2, learning_rate='auto', init='random', random_state=1113).fit_transform(data_t)\n",
        "plt.figure(figsize=(14,12))\n",
        "\n",
        "# plot t-SNE embedding in two-dimentional space\n",
        "ax_tsne = plt.subplot(221)\n",
        "plt.plot(X_embedded[:,0], X_embedded[:,1],'*')\n",
        "#plt.text(-20,9,'A',fontsize=20)\n",
        "\n",
        "# format aspect ratio and axis labels\n",
        "ax_tsne.set_aspect(1.5)\n",
        "ax_tsne.set_xlabel('t-SNE_1')\n",
        "ax_tsne.set_ylabel('t-SNE_2')\n",
        "ax_tsne.set_title('t-SNE results')\n",
        "\n",
        "# repeat plotting for further manual thresholding\n",
        "ax_tsne = plt.subplot(222)\n",
        "plt.plot(X_embedded[:,0], X_embedded[:,1],'*')\n",
        "ax_tsne.set_aspect(1.5)\n",
        "ax_tsne.set_xlabel('t-SNE_1')\n",
        "ax_tsne.set_ylabel('t-SNE_2')\n",
        "ax_tsne.set_title('t-SNE results with manual thresholding')\n",
        "#plt.xlim(-5, 5)\n",
        "#plt.text(-20,9,'B',fontsize=20)\n",
        "\n",
        "\n",
        "# manually split all elements by tSNE_1 = 2.5\n",
        "# np.where returns index of elements that satisfy given condition\n",
        "#can isolate cells using the threshold\n",
        "threshold = 2.5\n",
        "iis_red = np.where(X_embedded[:,0]<threshold);\n",
        "iis_blue = np.where(X_embedded[:,0]>=threshold);\n",
        "\n",
        "# label points left to the tSNE_1 = -5 line in red\n",
        "plt.scatter(X_embedded[iis_red,0], X_embedded[iis_red,1],80, 'r', marker='o')\n",
        "# demarcation line\n",
        "plt.plot([threshold, threshold],[-8, 8],'--k')\n",
        "\n",
        "# plot traces of all \"red\" cells\n",
        "ax1 = plt.subplot(223)\n",
        "plt.imshow(data_t[np.asarray(iis_red)[0],:], vmin=0,vmax=.50)\n",
        "ax1.set_aspect(20)\n",
        "ax1.set_title('Red cells')\n",
        "ax1.set_ylabel('Cell ID')\n",
        "ax1.set_xlabel('Frames')\n",
        "#plt.text(-200,-2,'C',fontsize=20)\n",
        "\n",
        "# plot traces of all \"non-red\" cells\n",
        "ax2 = plt.subplot(224)\n",
        "plt.imshow(data_t[np.asarray(iis_blue)[0],:], vmin=0,vmax=.50)\n",
        "ax2.set_aspect(6)\n",
        "ax2.set_title('Blue cells')\n",
        "ax2.set_ylabel('Cell ID')\n",
        "ax2.set_xlabel('Frames')\n",
        "#plt.text(-200,-2,'D',fontsize=20)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sebpfHMvnt-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare manual segmentation into red/blue cells and K-means clustering from earlier step\n",
        "# Plot historgram of cluster values for red cenns and blue cells\n",
        "plt.hist(labels[iis_blue], bins=range(Ks+1),align='left',rwidth=0.8,facecolor='b')\n",
        "plt.hist(labels[iis_red],  bins=range(Ks+1),align='left',rwidth=0.5,facecolor='r')\n",
        "# Red cells are exclusively present in clusters 0 and 3\n",
        "\n",
        "ax = plt.gca()\n",
        "ax.set_title('Number of red and blue cells per K-means cluster')\n",
        "ax.set_xlabel('Cluster')\n",
        "ax.set_ylabel('Number of cells')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YAs-pgot0K2Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extraneous Stuff"
      ],
      "metadata": {
        "id": "aL5MQXw9LgIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title toy dataframe\n",
        "frame_column = np.arange(0, 1200)\n",
        "\n",
        "# Generate random values for the other 10 columns\n",
        "data_values = np.random.uniform(0.01, 4.0, size=(1200, 100))\n",
        "\n",
        "# Create the dataframe\n",
        "df = pd.DataFrame(data_values, columns=[f'Column_{i}' for i in range(1, 101)])\n",
        "\n",
        "# Insert the 'Frame' column at the beginning\n",
        "df.insert(0, 'Frame', frame_column)\n",
        "\n",
        "# Display the dataframe\n",
        "print(df)"
      ],
      "metadata": {
        "id": "jEtg7mvtLtVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Power Analysis\n",
        "import scipy.stats as stats\n",
        "import numpy as np\n",
        "\n",
        "# Set the parameters for the power analysis\n",
        "effect_size = 0.15  # Anticipated effect size\n",
        "alpha = 0.01      # Significance level\n",
        "power_value = 0.8 # Desired power\n",
        "sigma = 0.15       # Population standard deviation\n",
        "alternative = 'larger'  # 'two-sided', 'larger', or 'smaller'\n",
        "\n",
        "# Calculate the Z score for the desired power (1 - alpha)\n",
        "z_power = stats.norm.ppf(1 - alpha)\n",
        "\n",
        "# Calculate the standard error (SE) using the population standard deviation (sigma)\n",
        "\n",
        "\n",
        "# Calculate the required sample size using the formula: n = ((z_power + stats.norm.ppf(1 - power_value)) * sigma / effect_size) ** 2\n",
        "n = ((z_power + stats.norm.ppf(1 - power_value)) * sigma / effect_size) ** 2\n",
        "se = sigma / np.sqrt(n)\n",
        "print(f\"Sample size required: {round(n)}\")"
      ],
      "metadata": {
        "id": "nashY91SLy7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Time Converter\n",
        "\"\"\"\n",
        "Converter is a function that I use to convert between the frames read out from the scientifica scope (e.g. 1860 frames), and the number of seconds or minutes in actual time.\n",
        "This is necssary because the scientifica scope records at 30.9 fps(31 FPS). When we convert the RAW files to tifs, we need to have them be a multiple of 31 or 30 in order\n",
        "to generate a hyperstack in fiji. You can either feed in number of frames to get the number of seconds/mins, or can feed it seconds to get the number of mins/frames, and same for mins.\n",
        "\n",
        "\n",
        "numframes: a float that represents the number of frames\n",
        "nummins = a float that represents the number of seconds\n",
        "numsecs = a float that represents the number of seconds you want to convert\n",
        "\"\"\"\n",
        "def converter(numframes=False, nummins=False, numsecs=False):\n",
        "    conversion = {\"Number of Frames\":[], \"Number of Minutes\":[], 'Number of Seconds':[]}\n",
        "    if type(numframes) == int or type(numframes) == float:\n",
        "        conversion['Number of Frames'].append(numframes)\n",
        "        mins = numframes/(FPS*Secs_in_Min)\n",
        "        conversion['Number of Minutes'].append(mins)\n",
        "        secs = numframes/FPS\n",
        "        conversion['Number of Seconds'].append(secs)\n",
        "    elif type(nummins) == int or type (nummins) == float:\n",
        "        conversion['Number of Minutes'].append(nummins)\n",
        "        frames = nummins *FPS*Secs_in_Min\n",
        "        conversion['Number of Frames'].append(frames)\n",
        "        secs = nummins *Secs_in_Min\n",
        "        conversion['Number of Seconds'].append(secs)\n",
        "    elif type(numsecs) == int or type(numsecs) == float:\n",
        "        conversion['Number of Seconds'].append(numsecs)\n",
        "        mins = numsecs/Secs_in_Min\n",
        "        conversion['Number of Minutes'].append(mins)\n",
        "        frames = numsecs*FPS\n",
        "        conversion['Number of Frames'].append(frames)\n",
        "    else:\n",
        "        print('you are in the wrong place at the wrong time >:]')\n",
        "\n",
        "    return conversion\n",
        "#dont feel like doing the math, here is the easy way to get the desired time measure you want\n",
        "a = converter(numframes = 3720)\n",
        "print(a)\n",
        "b = converter(numsecs = 806)\n",
        "print(b)\n",
        "c = converter(nummins =11)\n",
        "print(c)\n",
        "# 62 seconds per cycle"
      ],
      "metadata": {
        "id": "NuCaRs31Lz_D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOCqdsy9wzWlT2i7ZaXneac",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}